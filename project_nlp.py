# -*- coding: utf-8 -*-
"""project NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11j5toWa8YhmCyZrGxdsXJj0KqlOPaYey
"""

import pandas as pd
import numpy as np
from tensorflow import keras
from tqdm import tqdm
import nltk

from google.colab import drive
drive.mount('/content/drive')

print(keras.__version__)

data = pd.read_csv('/content/drive/MyDrive/data.csv')

data.head()

!pip install bs4
nltk.download('punkt')
nltk.download('wordnet')

nltk.download('stopwords')
nltk.download('omw-1.4')

data['title']

from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer 
import re
lm=WordNetLemmatizer()


def ReturnCleanText(text):
        text = re.sub('[^a-zA-Z]', ' ', text)
        # change the text into lower case.(Note: in case of social media text, it is good to leave them as it is)
        text = text.lower()
        # removing xml tags from tweets
        text =BeautifulSoup(text, 'lxml').get_text()
        # removing URLS 
        text =re.sub('https?://[A-Za-z0-9./]+','',text)
        # removing words with "@"
        text =re.sub(r'@[A-Za-z0-9]+','',text)
        # removing special characters
        text = re.sub(r"\W+|_", ' ', text)
        # tokenization of sentences
        text = word_tokenize(text)
        # lemmatize the text using WordNetn
        words = [lm.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]   
        return " ".join(words)
    
data['title'] = data['title'].apply(ReturnCleanText)

data.head()

import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras import utils
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization


from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout

max_features = 2000
Encoder = keras.layers.experimental.preprocessing.TextVectorization( max_tokens = max_features)
Encoder.adapt(data['text'].values)

vocab = np.array(Encoder.get_vocabulary())
print(vocab[:20])

example ="This is an example to test the encoder that we just created!"
print(Encoder(example).numpy())
print(" ".join(vocab[Encoder(example).numpy()]))

max_features = 2000
tokenizer = Tokenizer(num_words = max_features, )
tokenizer.fit_on_texts(data['text'].values)
X = tokenizer.texts_to_sequences(data['text'].values)
X = pad_sequences(X, padding = 'post' ,maxlen=300)
Y = pd.get_dummies(data['target']).values

vocab_size = len(tokenizer.word_index)+1

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

embid_dim = 300
lstm_out = 128


model = keras.Sequential()
model.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))
model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))
model.add(Dense(128, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(2, activation = 'sigmoid'))
model.summary()

batch_size = 128
model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))

model.save('bidirectional.h5')

"""**Word2Vec**"""

sentences =[]
for t in  tqdm(range(len(data['text']))):
    text = nltk.word_tokenize(data['text'][t])
    sentences.append(text)

from gensim.models import Word2Vec
w2v_model = Word2Vec(sentences, size=300, min_count=2, sg = 0 )

words = list(w2v_model.wv.vocab)
print('Vocabulary size: %d' % len(words))

# save model 
filename = 'embedding_word2vec.txt'
w2v_model.wv.save_word2vec_format(filename, binary=False)

embedding_vector = {}
f = open('embedding_word2vec.txt')
for line in tqdm(f):
    value = line.split(' ')
    word = value[0]
    coef = np.array(value[1:],dtype = 'float32')
    embedding_vector[word] = coef

embedding_matrix = np.zeros((vocab_size,300))
for word,i in tqdm(tokenizer.word_index.items()):
    embedding_value = embedding_vector.get(word)
    if embedding_value is not None:
        embedding_matrix[i] = embedding_value

embid_dim = 300
lstm_out = 128


model = keras.Sequential()
model.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [ embedding_matrix] , trainable = False))
model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))
model.add(Dense(128, activation = 'relu')) 
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(2, activation = 'sigmoid'))
model.summary()

batch_size = 128
model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))

model.save('word2vecmodel.h5')